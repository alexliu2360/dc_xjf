{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "def tpr_weight_funtion(y_true,y_predict):\n",
    "    d = pd.DataFrame()\n",
    "    d['prob'] = list(y_predict)\n",
    "    d['y'] = list(y_true)\n",
    "    d = d.sort_values(['prob'], ascending=[0])\n",
    "    y = d.y\n",
    "    PosAll = pd.Series(y).value_counts()[1]\n",
    "    NegAll = pd.Series(y).value_counts()[0]\n",
    "    pCumsum = d['y'].cumsum()\n",
    "    nCumsum = np.arange(len(y)) - pCumsum + 1\n",
    "    pCumsumPer = pCumsum / PosAll\n",
    "    nCumsumPer = nCumsum / NegAll\n",
    "    TR1 = pCumsumPer[abs(nCumsumPer-0.001).idxmin()]\n",
    "    TR2 = pCumsumPer[abs(nCumsumPer-0.005).idxmin()]\n",
    "    TR3 = pCumsumPer[abs(nCumsumPer-0.01).idxmin()]\n",
    "    return 'TC_AUC',0.4 * TR1 + 0.3 * TR2 + 0.3 * TR3,True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = '../data/'\n",
    "origin_path = data_path + 'origin_data/'\n",
    "fts_path = data_path + 'fts/'\n",
    "\n",
    "op_train_new_fn = 'operation_train_new.csv'\n",
    "tran_train_new_fn = 'transaction_train_new.csv'\n",
    "tag_train_new_fn = 'tag_train_new.csv'\n",
    "op_train_sorted_fn = 'op_train_sorted.csv'\n",
    "tran_train_sorted_fn = 'tran_train_sorted.csv'\n",
    "tag_train_sorted_fn = 'tag_train_sorted.csv'\n",
    "\n",
    "op_origin_fn = 'op_origin.csv'\n",
    "tran_origin_fn = 'tran_origin.csv'\n",
    "tag_train_fts_fn = 'tag_fts.csv'\n",
    "round1_fts_fn = 'round1_fts.csv'\n",
    "\n",
    "op_train_new_file = data_path + op_train_new_fn\n",
    "tran_train_new_file = data_path + tran_train_new_fn\n",
    "tag_train_new_file = data_path + tag_train_new_fn\n",
    "op_train_sorted_file = data_path + op_train_sorted_fn\n",
    "tran_train_sorted_file = data_path + tran_train_sorted_fn\n",
    "tag_train_sorted_file = data_path + tag_train_sorted_fn\n",
    "\n",
    "op_origin_file = origin_path + op_origin_fn\n",
    "tran_origin_file = origin_path + tran_origin_fn\n",
    "tag_train_fts_file = fts_path + tag_train_fts_fn\n",
    "round1_fts_file = fts_path + round1_fts_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print('[info]:start read from op_train...')\n",
    "    op_train = pd.read_csv(op_origin_file).drop('Unnamed: 0', axis=1)\n",
    "    print('[info]:start read from tran_train...')\n",
    "    tran_train = pd.read_csv(tran_origin_file).drop('Unnamed: 0', axis=1)\n",
    "    print('[info]:start read from tag_train...')\n",
    "    tag_train = pd.read_csv(tag_train_sorted_file).drop('Unnamed: 0', axis=1)\n",
    "    return op_train, tran_train, tag_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_rd1 = pd.read_csv('../data/operation_round1_new.csv')\n",
    "tran_rd1 = pd.read_csv('../data/transaction_round1_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UID    31198\n",
       "Tag    31198\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.read_csv('../data/submit_sample.csv')\n",
    "s.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_test_data():\n",
    "    train_data = pd.read_csv(tag_train_fts_file)\n",
    "    test_data = pd.read_csv(round1_fts_file)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 引入train test数据\n",
    "train_data, test_data = load_train_test_data()\n",
    "train_data.drop(['Unnamed: 0'],axis=1, inplace=True)\n",
    "test_data.drop(['Unnamed: 0'],axis=1, inplace=True)\n",
    "train_data.fillna(-1, inplace=True)\n",
    "test_data.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = train_data['Tag']\n",
    "train = train_data.drop(['UID','Tag'],axis = 1)\n",
    "test_id = test_data['UID']\n",
    "test = test_data.drop(['UID','Tag'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.253935\tvalid_1's binary_logloss: 0.260542\n",
      "[100]\tvalid_0's binary_logloss: 0.193541\tvalid_1's binary_logloss: 0.20644\n",
      "[150]\tvalid_0's binary_logloss: 0.158737\tvalid_1's binary_logloss: 0.177571\n",
      "[200]\tvalid_0's binary_logloss: 0.135382\tvalid_1's binary_logloss: 0.159636\n",
      "[250]\tvalid_0's binary_logloss: 0.118698\tvalid_1's binary_logloss: 0.148342\n",
      "[300]\tvalid_0's binary_logloss: 0.106012\tvalid_1's binary_logloss: 0.140944\n",
      "[350]\tvalid_0's binary_logloss: 0.0958368\tvalid_1's binary_logloss: 0.135587\n",
      "[400]\tvalid_0's binary_logloss: 0.0875051\tvalid_1's binary_logloss: 0.132035\n",
      "[450]\tvalid_0's binary_logloss: 0.0802733\tvalid_1's binary_logloss: 0.129308\n",
      "[500]\tvalid_0's binary_logloss: 0.0740833\tvalid_1's binary_logloss: 0.127359\n",
      "[550]\tvalid_0's binary_logloss: 0.0686329\tvalid_1's binary_logloss: 0.12591\n",
      "[600]\tvalid_0's binary_logloss: 0.0638399\tvalid_1's binary_logloss: 0.12481\n",
      "[650]\tvalid_0's binary_logloss: 0.0596082\tvalid_1's binary_logloss: 0.124021\n",
      "[700]\tvalid_0's binary_logloss: 0.0557946\tvalid_1's binary_logloss: 0.1232\n",
      "[750]\tvalid_0's binary_logloss: 0.0523811\tvalid_1's binary_logloss: 0.122697\n",
      "[800]\tvalid_0's binary_logloss: 0.0493405\tvalid_1's binary_logloss: 0.122236\n",
      "[850]\tvalid_0's binary_logloss: 0.0465799\tvalid_1's binary_logloss: 0.12194\n",
      "[900]\tvalid_0's binary_logloss: 0.044062\tvalid_1's binary_logloss: 0.121891\n",
      "[950]\tvalid_0's binary_logloss: 0.0417743\tvalid_1's binary_logloss: 0.121897\n",
      "Early stopping, best iteration is:\n",
      "[934]\tvalid_0's binary_logloss: 0.0424751\tvalid_1's binary_logloss: 0.121758\n",
      "[0.12175848266216446]\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.254598\tvalid_1's binary_logloss: 0.260353\n",
      "[100]\tvalid_0's binary_logloss: 0.194909\tvalid_1's binary_logloss: 0.205174\n",
      "[150]\tvalid_0's binary_logloss: 0.160289\tvalid_1's binary_logloss: 0.174882\n",
      "[200]\tvalid_0's binary_logloss: 0.136859\tvalid_1's binary_logloss: 0.156041\n",
      "[250]\tvalid_0's binary_logloss: 0.120094\tvalid_1's binary_logloss: 0.143693\n",
      "[300]\tvalid_0's binary_logloss: 0.107373\tvalid_1's binary_logloss: 0.135923\n",
      "[350]\tvalid_0's binary_logloss: 0.0971588\tvalid_1's binary_logloss: 0.130595\n",
      "[400]\tvalid_0's binary_logloss: 0.0887082\tvalid_1's binary_logloss: 0.126716\n",
      "[450]\tvalid_0's binary_logloss: 0.0814055\tvalid_1's binary_logloss: 0.12367\n",
      "[500]\tvalid_0's binary_logloss: 0.0752636\tvalid_1's binary_logloss: 0.121561\n",
      "[550]\tvalid_0's binary_logloss: 0.0696999\tvalid_1's binary_logloss: 0.119628\n",
      "[600]\tvalid_0's binary_logloss: 0.0648524\tvalid_1's binary_logloss: 0.118467\n",
      "[650]\tvalid_0's binary_logloss: 0.0604709\tvalid_1's binary_logloss: 0.117355\n",
      "[700]\tvalid_0's binary_logloss: 0.0566324\tvalid_1's binary_logloss: 0.116732\n",
      "[750]\tvalid_0's binary_logloss: 0.0531897\tvalid_1's binary_logloss: 0.11605\n",
      "[800]\tvalid_0's binary_logloss: 0.0500596\tvalid_1's binary_logloss: 0.115627\n",
      "[850]\tvalid_0's binary_logloss: 0.0472241\tvalid_1's binary_logloss: 0.115179\n",
      "[900]\tvalid_0's binary_logloss: 0.044699\tvalid_1's binary_logloss: 0.114859\n",
      "[950]\tvalid_0's binary_logloss: 0.042392\tvalid_1's binary_logloss: 0.114694\n",
      "[1000]\tvalid_0's binary_logloss: 0.0402884\tvalid_1's binary_logloss: 0.114682\n",
      "[1050]\tvalid_0's binary_logloss: 0.0384091\tvalid_1's binary_logloss: 0.114622\n",
      "[1100]\tvalid_0's binary_logloss: 0.0366528\tvalid_1's binary_logloss: 0.114527\n",
      "[1150]\tvalid_0's binary_logloss: 0.0350428\tvalid_1's binary_logloss: 0.114519\n",
      "Early stopping, best iteration is:\n",
      "[1120]\tvalid_0's binary_logloss: 0.0359867\tvalid_1's binary_logloss: 0.114491\n",
      "[0.12175848266216446, 0.11449064454272397]\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.253639\tvalid_1's binary_logloss: 0.26587\n",
      "[100]\tvalid_0's binary_logloss: 0.193336\tvalid_1's binary_logloss: 0.213573\n",
      "[150]\tvalid_0's binary_logloss: 0.158196\tvalid_1's binary_logloss: 0.185359\n",
      "[200]\tvalid_0's binary_logloss: 0.134428\tvalid_1's binary_logloss: 0.168083\n",
      "[250]\tvalid_0's binary_logloss: 0.117296\tvalid_1's binary_logloss: 0.156912\n",
      "[300]\tvalid_0's binary_logloss: 0.104406\tvalid_1's binary_logloss: 0.149798\n",
      "[350]\tvalid_0's binary_logloss: 0.0940952\tvalid_1's binary_logloss: 0.144904\n",
      "[400]\tvalid_0's binary_logloss: 0.0855271\tvalid_1's binary_logloss: 0.141376\n",
      "[450]\tvalid_0's binary_logloss: 0.0782294\tvalid_1's binary_logloss: 0.138739\n",
      "[500]\tvalid_0's binary_logloss: 0.0721015\tvalid_1's binary_logloss: 0.137033\n",
      "[550]\tvalid_0's binary_logloss: 0.0666469\tvalid_1's binary_logloss: 0.135547\n",
      "[600]\tvalid_0's binary_logloss: 0.0619698\tvalid_1's binary_logloss: 0.134585\n",
      "[650]\tvalid_0's binary_logloss: 0.0577366\tvalid_1's binary_logloss: 0.133945\n",
      "[700]\tvalid_0's binary_logloss: 0.054058\tvalid_1's binary_logloss: 0.133454\n",
      "[750]\tvalid_0's binary_logloss: 0.0507266\tvalid_1's binary_logloss: 0.133152\n",
      "[800]\tvalid_0's binary_logloss: 0.0477767\tvalid_1's binary_logloss: 0.132901\n",
      "[850]\tvalid_0's binary_logloss: 0.0451119\tvalid_1's binary_logloss: 0.132791\n",
      "Early stopping, best iteration is:\n",
      "[836]\tvalid_0's binary_logloss: 0.0458259\tvalid_1's binary_logloss: 0.132758\n",
      "[0.12175848266216446, 0.11449064454272397, 0.13275786846799248]\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.253028\tvalid_1's binary_logloss: 0.263285\n",
      "[100]\tvalid_0's binary_logloss: 0.19278\tvalid_1's binary_logloss: 0.210966\n",
      "[150]\tvalid_0's binary_logloss: 0.158017\tvalid_1's binary_logloss: 0.182908\n",
      "[200]\tvalid_0's binary_logloss: 0.134387\tvalid_1's binary_logloss: 0.165211\n",
      "[250]\tvalid_0's binary_logloss: 0.117503\tvalid_1's binary_logloss: 0.15437\n",
      "[300]\tvalid_0's binary_logloss: 0.104777\tvalid_1's binary_logloss: 0.147106\n",
      "[350]\tvalid_0's binary_logloss: 0.0946829\tvalid_1's binary_logloss: 0.141927\n",
      "[400]\tvalid_0's binary_logloss: 0.086285\tvalid_1's binary_logloss: 0.138297\n",
      "[450]\tvalid_0's binary_logloss: 0.0791797\tvalid_1's binary_logloss: 0.135668\n",
      "[500]\tvalid_0's binary_logloss: 0.0730247\tvalid_1's binary_logloss: 0.13361\n",
      "[550]\tvalid_0's binary_logloss: 0.0675854\tvalid_1's binary_logloss: 0.132026\n",
      "[600]\tvalid_0's binary_logloss: 0.0627959\tvalid_1's binary_logloss: 0.130918\n",
      "[650]\tvalid_0's binary_logloss: 0.0585073\tvalid_1's binary_logloss: 0.129959\n",
      "[700]\tvalid_0's binary_logloss: 0.0547759\tvalid_1's binary_logloss: 0.129374\n",
      "[750]\tvalid_0's binary_logloss: 0.0514252\tvalid_1's binary_logloss: 0.128818\n",
      "[800]\tvalid_0's binary_logloss: 0.0484065\tvalid_1's binary_logloss: 0.128583\n",
      "[850]\tvalid_0's binary_logloss: 0.0457106\tvalid_1's binary_logloss: 0.128451\n",
      "[900]\tvalid_0's binary_logloss: 0.0432638\tvalid_1's binary_logloss: 0.128239\n",
      "[950]\tvalid_0's binary_logloss: 0.0410502\tvalid_1's binary_logloss: 0.128066\n",
      "Early stopping, best iteration is:\n",
      "[945]\tvalid_0's binary_logloss: 0.0412682\tvalid_1's binary_logloss: 0.128051\n",
      "[0.12175848266216446, 0.11449064454272397, 0.13275786846799248, 0.12805145757469258]\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.254405\tvalid_1's binary_logloss: 0.259277\n",
      "[100]\tvalid_0's binary_logloss: 0.194477\tvalid_1's binary_logloss: 0.205737\n",
      "[150]\tvalid_0's binary_logloss: 0.159537\tvalid_1's binary_logloss: 0.176746\n",
      "[200]\tvalid_0's binary_logloss: 0.136282\tvalid_1's binary_logloss: 0.159177\n",
      "[250]\tvalid_0's binary_logloss: 0.119546\tvalid_1's binary_logloss: 0.147792\n",
      "[300]\tvalid_0's binary_logloss: 0.10678\tvalid_1's binary_logloss: 0.140183\n",
      "[350]\tvalid_0's binary_logloss: 0.0966678\tvalid_1's binary_logloss: 0.134999\n",
      "[400]\tvalid_0's binary_logloss: 0.0883411\tvalid_1's binary_logloss: 0.131025\n",
      "[450]\tvalid_0's binary_logloss: 0.0811846\tvalid_1's binary_logloss: 0.127939\n",
      "[500]\tvalid_0's binary_logloss: 0.0749814\tvalid_1's binary_logloss: 0.125667\n",
      "[550]\tvalid_0's binary_logloss: 0.0695034\tvalid_1's binary_logloss: 0.123957\n",
      "[600]\tvalid_0's binary_logloss: 0.0647044\tvalid_1's binary_logloss: 0.122675\n",
      "[650]\tvalid_0's binary_logloss: 0.0603843\tvalid_1's binary_logloss: 0.121779\n",
      "[700]\tvalid_0's binary_logloss: 0.0565555\tvalid_1's binary_logloss: 0.121036\n",
      "[750]\tvalid_0's binary_logloss: 0.0530649\tvalid_1's binary_logloss: 0.120409\n",
      "[800]\tvalid_0's binary_logloss: 0.0499449\tvalid_1's binary_logloss: 0.119895\n",
      "[850]\tvalid_0's binary_logloss: 0.0471717\tvalid_1's binary_logloss: 0.11954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[900]\tvalid_0's binary_logloss: 0.0446625\tvalid_1's binary_logloss: 0.119177\n",
      "[950]\tvalid_0's binary_logloss: 0.0423823\tvalid_1's binary_logloss: 0.119023\n",
      "[1000]\tvalid_0's binary_logloss: 0.0402906\tvalid_1's binary_logloss: 0.119035\n",
      "Early stopping, best iteration is:\n",
      "[951]\tvalid_0's binary_logloss: 0.0423382\tvalid_1's binary_logloss: 0.119013\n",
      "[0.12175848266216446, 0.11449064454272397, 0.13275786846799248, 0.12805145757469258, 0.11901303946417432]\n"
     ]
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=100, reg_alpha=0.0, reg_lambda=1, max_depth=-1,\n",
    "    n_estimators=5000, objective='binary', subsample=0.7, colsample_bytree=0.77, subsample_freq=1, learning_rate=0.01,\n",
    "    random_state=2018, n_jobs=50, min_child_weight=4, min_child_samples=5, min_split_gain=0)\n",
    "skf = StratifiedKFold(n_splits=5, random_state=2018, shuffle=True)\n",
    "best_score = []\n",
    "\n",
    "oof_preds = np.zeros(train.shape[0])\n",
    "sub_preds = np.zeros(test_id.shape[0])\n",
    "\n",
    "for index, (train_index, test_index) in enumerate(skf.split(train, label)):\n",
    "    lgb_model.fit(train.iloc[train_index], label.iloc[train_index], verbose=50,\n",
    "                  eval_set=[(train.iloc[train_index], label.iloc[train_index]),\n",
    "                            (train.iloc[test_index], label.iloc[test_index])], early_stopping_rounds=50)\n",
    "    best_score.append(lgb_model.best_score_['valid_1']['binary_logloss'])\n",
    "    print(best_score)\n",
    "    oof_preds[test_index] = lgb_model.predict_proba(train.iloc[test_index], num_iteration=lgb_model.best_iteration_)[:,1]\n",
    "\n",
    "    test_pred = lgb_model.predict_proba(test, num_iteration=lgb_model.best_iteration_)[:, 1]\n",
    "    sub_preds += test_pred / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7321820303383897\n"
     ]
    }
   ],
   "source": [
    "m = tpr_weight_funtion(y_predict=oof_preds,y_true=label)\n",
    "print(m[1])\n",
    "submit = pd.read_csv('../data/submit_sample.csv')\n",
    "submit['Tag'] = sub_preds\n",
    "submit.to_csv('../data/sub/baseline_%s.csv'%str(m),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.read_csv('../data/sub/baseline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31198, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "op_train, tran_train, tag_train = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_mac1_dict = op_train['mac1'].value_counts().to_dict()\n",
    "op_ipsub_dict = op_train['ipsub'].value_counts().to_dict()\n",
    "tran_mac1_dict = tran_train['mac1'].value_counts().to_dict()\n",
    "tran_ip1sub_dict = tran_train['ip1_sub'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag(uid):\n",
    "    return tag_train[tag_train['UID'] == uid]\n",
    "\n",
    "def get_op(uid):\n",
    "    return op_train[op_train['UID'] == uid]\n",
    "\n",
    "def get_tran(uid):\n",
    "    return tran_train[tran_train['UID'] == uid]\n",
    "\n",
    "def get_value_counts(uid, train_data):\n",
    "    assert type(train_data) is pd.DataFrame\n",
    "    for c in list(train_data.columns):\n",
    "        print('[%r]'%c)\n",
    "        print(train_data[train_data['UID']==uid][c].value_counts())\n",
    "        print('====')\n",
    "        \n",
    "def oneday_cnt(tmp, gb_str, ft_str):\n",
    "    gb = tmp.groupby(gb_str)\n",
    "    top_oneday = []\n",
    "    for gb_key in gb.indices.keys():\n",
    "        cnt = 0\n",
    "        sub_gb = gb.get_group(gb_key)\n",
    "        value_counts = sub_gb[ft_str].value_counts()\n",
    "        if not value_counts.empty:\n",
    "            top_value = value_counts.sort_values(ascending=False).values[0]\n",
    "            top_mode = value_counts.sort_values(ascending=False).index[0]\n",
    "            top_oneday.append([gb_key, top_value, top_mode])\n",
    "    if not len(top_oneday):\n",
    "        top_oneday = [np.nan, np.nan, np.nan]\n",
    "    return top_oneday\n",
    "\n",
    "def topcnts_oneday(tmp, sub_str):\n",
    "    top_oneday = oneday_cnt(tmp, 'day', sub_str)\n",
    "    top_idx, top_cnt = 0, top_oneday[0][1]\n",
    "    for item in top_oneday:\n",
    "        if top_cnt < item[1]:\n",
    "            top_cnt = item[1]\n",
    "            top_idx += 1\n",
    "    top_day = top_oneday[top_idx][0]\n",
    "    top_value = top_oneday[top_idx][1]\n",
    "    top_mode = top_oneday[top_idx][2]\n",
    "    return top_day, top_value, top_mode\n",
    "        \n",
    "def op_times_per2min(tmp):\n",
    "    day_gb = tmp.groupby('day')\n",
    "    min2 = 120\n",
    "    over2min_rec = []\n",
    "    for gb_key in day_gb.indices.keys():\n",
    "        over2min_cnt = 0\n",
    "        sgb = day_gb.get_group(gb_key)\n",
    "        timestamp = list(sgb['timestamp'].get_values())\n",
    "        index, idx_max = 0, len(timestamp) - 1\n",
    "        now_t = timestamp[index]\n",
    "        for t in timestamp:\n",
    "            if timestamp[index] - now_t > min2:\n",
    "                now_t = timestamp[index]\n",
    "                over2min_cnt += 1\n",
    "            index += 1\n",
    "        over2min_rec.append(over2min_cnt)\n",
    "    return max(over2min_rec)\n",
    "\n",
    "def suc_rate(tmp):\n",
    "    for item in tmp['success'].value_counts().items():\n",
    "        if item[0] == 1:\n",
    "            suc_cnt = item[1]\n",
    "            break\n",
    "    suc_all = tmp['success'].count()\n",
    "    return suc_cnt/suc_all\n",
    "\n",
    "def fbfill_series(series):\n",
    "    series = series.copy()\n",
    "#     series.replace(0, np.nan, inplace=True)\n",
    "    new_s = series.fillna(method='bfill')\n",
    "    new_s = series.fillna(method='ffill')\n",
    "    return new_s\n",
    "\n",
    "def ft_change_cnt(tmp, ft_str):   \n",
    "    assert ft_str in tmp.keys()\n",
    "    \n",
    "    # 排除某一个ft下全部是NaN的情况\n",
    "    if len(tmp[ft_str][tmp[ft_str].notna()]):\n",
    "        series = fbfill_series(tmp[ft_str])\n",
    "        last_dc = series.get_values()[0]\n",
    "        dc_cnt = 0\n",
    "        for dc in series.get_values():\n",
    "            if last_dc != dc:\n",
    "                last_dc = dc\n",
    "                dc_cnt += 1\n",
    "        return dc_cnt\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def get_change_frq(tmp, ft_str):\n",
    "    frq = ft_change_cnt(tmp, ft_str)/tmp['day'].count()\n",
    "    if frq is not np.nan:\n",
    "        return float('%.2f' % frq)\n",
    "    else:\n",
    "        return np.nan\n",
    "       \n",
    "def ip_change_oneday_top(tmp, ip_ft_str):   \n",
    "    day_gb = tmp.groupby('day')\n",
    "    ip_rec = []\n",
    "    for gb_key in day_gb.indices.keys():\n",
    "        sgb = day_gb.get_group(gb_key)\n",
    "        series = fbfill_series(sgb[ip_ft_str])\n",
    "        last_ip = series.get_values()[0]\n",
    "        ip_cnt = 0\n",
    "        if np.isnan(last_ip):\n",
    "            ip_rec.append(np.nan)\n",
    "            continue\n",
    "        for ip in series.get_values():\n",
    "            if last_ip != ip:\n",
    "                last_ip = ip\n",
    "                ip_cnt += 1\n",
    "        ip_rec.append(ip_cnt)\n",
    "    return max(ip_rec)\n",
    "\n",
    "def top_type(tmp, ft_str):\n",
    "    value_counts = tmp[ft_str].value_counts()\n",
    "    if not value_counts.empty:\n",
    "        return value_counts.sort_values(ascending=False).index[0]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def top_value(tmp, ft_str):\n",
    "    value_counts = tmp[ft_str].value_counts()\n",
    "    if not value_counts.empty:\n",
    "        return value_counts.sort_values(ascending=False).values[0]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def get_frq(tmp, ft_str):\n",
    "    return float('%.2f' % (top_value(tmp, ft_str)/tmp['day'].count()))\n",
    "\n",
    "def top_op_mac1_in_diffUID_cnt(tmp):\n",
    "    tp = top_type(tmp, 'mac1')\n",
    "    if tp and tp in op_mac1_dict.keys():\n",
    "        return op_mac1_dict[tp]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def top_type_in_diffUID_cnt(tmp, ft_str, type_dict):\n",
    "    tp = top_type(tmp, ft_str)\n",
    "    if tp and tp in type_dict.keys():\n",
    "        return type_dict[tp]\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "op_gb = op_train.groupby('UID')\n",
    "drop_fts = ['mode','success','time','device1','device2','device_code1','device_code2','device_code3','mac1','ip1','ip2','ip1_sub','ip2_sub','timestamp']\n",
    "op_train_nf = op_gb.count().drop(drop_fts, axis='columns')\n",
    "op_train_nf.rename(columns={'day':'day_cnts'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_10001 = get_op(10001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# op\n",
    "op_feature = {}\n",
    "op_feature['UID'] = op_10001['UID'].values[0]\n",
    "op_feature['day_cnts'] = op_10001['day'].count()\n",
    "op_feature['op_top_appear_day'] = top_type(op_10001, 'day')\n",
    "op_feature['op_top_appear_day_cnt'] = top_value(op_10001, 'day')\n",
    "op_feature['op_times_per2min'] = op_times_per2min(op_10001)\n",
    "op_feature['mode_top_day_oneday'] = topcnts_oneday(op_10001, 'mode')[0] # 一天中某一操作类型次数最多的那一天\n",
    "op_feature['mode_top_cnt_oneday'] = topcnts_oneday(op_10001, 'mode')[1] # 一天中某一操作类型次数最多的次数\n",
    "op_feature['mode_top_type_oneday'] = topcnts_oneday(op_10001, 'mode')[2] # 一天中某一操作类型次数最多的类型\n",
    "op_feature['mode_cnt'] = top_value(op_10001, 'mode')\n",
    "op_feature['mode_rank1'] = top_type(op_10001, 'mode')\n",
    "op_feature['suc_rate'] = '%.2f' % (suc_rate(op_10001))\n",
    "op_feature['device_code_frq'] = get_change_frq(op_10001, 'device_code')\n",
    "op_feature['ip_change_frq'] = get_change_frq(op_10001, 'ip')\n",
    "op_feature['ip_change_oneday_top'] = ip_change_oneday_top(op_10001, 'ip')\n",
    "op_feature['ip_change_cnt'] = ft_change_cnt(op_10001, 'ip')\n",
    "op_feature['top_mac1_in_diffUID_cnt'] = top_type_in_diffUID_cnt(op_10001, 'mac1', op_mac1_dict)\n",
    "op_feature['top_ipsub_in_diffUID_cnt'] = top_type_in_diffUID_cnt(op_10001, 'ipsub', op_ipsub_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "features.append(op_feature)\n",
    "features = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_train_nf = op_train_nf.join(features)\n",
    "op_train_nf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_10001 = get_tran(10001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_10001 = get_tran(17520)\n",
    "tran_feature = {}\n",
    "tran_feature['UID'] = tran_10001['UID'].values[0]\n",
    "tran_feature['channel_top'] = top_type(tran_10001, 'channel')\n",
    "tran_feature['channel_top_frq'] = top_value(tran_10001, 'channel')\n",
    "tran_feature['tran_day_cnts'] = tran_10001['day'].count()\n",
    "tran_feature['tran_day_appear_top'] = top_type(tran_10001, 'day')\n",
    "tran_feature['tran_amt_frq'] = get_frq(tran_10001, 'trans_amt')\n",
    "tran_feature['tran_amt_top'] = top_type(tran_10001, 'trans_amt')\n",
    "tran_feature['tran_topcnts_oneday'] = topcnts_oneday(tran_10001, 'trans_amt')[1]\n",
    "tran_feature['tran_times_per2min'] = op_times_per2min(tran_10001)\n",
    "tran_feature['amt_src1_frq'] = get_change_frq(tran_10001, 'amt_src1')\n",
    "tran_feature['amt_src1_type_top'] = top_type(tran_10001, 'amt_src1')\n",
    "tran_feature['amt_src1_type_cnt'] = topcnts_oneday(tran_10001, 'amt_src1')[1]\n",
    "tran_feature['amt_src2_frq'] = get_change_frq(tran_10001, 'amt_src2')\n",
    "tran_feature['amt_src2_type_top'] = top_type(tran_10001, 'amt_src2')\n",
    "tran_feature['amt_src2_type_cnt'] = topcnts_oneday(tran_10001, 'amt_src2')[1]\n",
    "tran_feature['merchant_frq'] = get_change_frq(tran_10001, 'merchant')\n",
    "tran_feature['merchant_type_top'] = top_type(tran_10001, 'merchant')\n",
    "tran_feature['merchant_type_cnt'] = len(tran_10001['merchant'].value_counts()) # 商户标识类型总数\n",
    "tran_feature['code1_type_top'] = top_type(tran_10001, 'code1')\n",
    "tran_feature['code1_type_cnt'] = len(tran_10001['code1'].value_counts()) # 出现最多的商户子门店\n",
    "tran_feature['trans_type1_top_cnt'] = top_value(tran_10001, 'trans_type1')\n",
    "tran_feature['trans_type1_top_frq'] = get_frq(tran_10001, 'trans_type1')\n",
    "tran_feature['trans_type1_top'] = top_type(tran_10001, 'trans_type1')\n",
    "tran_feature['trans_type2_top_cnt'] = top_value(tran_10001, 'trans_type2')\n",
    "tran_feature['trans_type2_top_frq'] = get_frq(tran_10001, 'trans_type2')\n",
    "tran_feature['trans_type2_top'] = top_type(tran_10001, 'trans_type2')\n",
    "tran_feature['acc_id1_top_cnt'] = top_value(tran_10001, 'acc_id1')\n",
    "tran_feature['acc_id1_top_frq'] = get_frq(tran_10001, 'acc_id1')\n",
    "tran_feature['acc_id1_top'] = top_type(tran_10001, 'acc_id1')\n",
    "tran_feature['device_code_frq'] = get_change_frq(tran_10001, 'device_code')\n",
    "tran_feature['dev_name_frq'] = get_change_frq(tran_10001, 'device1')\n",
    "tran_feature['dev_type_frq'] = get_change_frq(tran_10001, 'device2')\n",
    "tran_feature['ip_change_oneday_top'] = ip_change_oneday_top(tran_10001, 'ip1')\n",
    "tran_feature['ip_change_frq'] = get_change_frq(tran_10001, 'ip1') # ip变化次数\n",
    "tran_feature['ip_change_times'] = ft_change_cnt(tran_10001, 'ip1') # ip变化次数\n",
    "tran_feature['top_mac1_in_diffUID_cnt'] = top_type_in_diffUID_cnt(tran_10001, 'mac1', tran_mac1_dict)\n",
    "tran_feature['top_ip1sub_in_diffUID_cnt'] = top_type_in_diffUID_cnt(tran_10001, 'ip1_sub', tran_ip1sub_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tran_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(op,trans,label):\n",
    "    for feature in op.columns[2:]:\n",
    "        label = label.merge(op.groupby(['UID'])[feature].count().reset_index(),on='UID',how='left')\n",
    "        label =label.merge(op.groupby(['UID'])[feature].nunique().reset_index(),on='UID',how='left')\n",
    "    \n",
    "    for feature in trans.columns[2:]:\n",
    "        if trans_train[feature].dtype == 'object':\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].count().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].nunique().reset_index(),on='UID',how='left')\n",
    "        else:\n",
    "            print(feature)\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].count().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].nunique().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].max().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].min().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].sum().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].mean().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].std().reset_index(),on='UID',how='left')\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_feature(op_train,trans_train,y)\n",
    "test = get_feature(op_test,trans_test,sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['UID','Tag'],axis = 1).fillna(-1)\n",
    "label = y['Tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test['UID']\n",
    "test = test.drop(['UID','Tag'],axis = 1).fillna(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

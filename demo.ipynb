{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "def tpr_weight_funtion(y_true,y_predict):\n",
    "    d = pd.DataFrame()\n",
    "    d['prob'] = list(y_predict)\n",
    "    d['y'] = list(y_true)\n",
    "    d = d.sort_values(['prob'], ascending=[0])\n",
    "    y = d.y\n",
    "    PosAll = pd.Series(y).value_counts()[1]\n",
    "    NegAll = pd.Series(y).value_counts()[0]\n",
    "    pCumsum = d['y'].cumsum()\n",
    "    nCumsum = np.arange(len(y)) - pCumsum + 1\n",
    "    pCumsumPer = pCumsum / PosAll\n",
    "    nCumsumPer = nCumsum / NegAll\n",
    "    TR1 = pCumsumPer[abs(nCumsumPer-0.001).idxmin()]\n",
    "    TR2 = pCumsumPer[abs(nCumsumPer-0.005).idxmin()]\n",
    "    TR3 = pCumsumPer[abs(nCumsumPer-0.01).idxmin()]\n",
    "    return 'TC_AUC',0.4 * TR1 + 0.3 * TR2 + 0.3 * TR3,True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = '../data/'\n",
    "origin_path = data_path + 'origin_data/'\n",
    "fts_path = data_path + 'fts/'\n",
    "\n",
    "op_train_new_fn = 'operation_train_new.csv'\n",
    "tran_train_new_fn = 'transaction_train_new.csv'\n",
    "tag_train_new_fn = 'tag_train_new.csv'\n",
    "op_train_sorted_fn = 'op_train_sorted.csv'\n",
    "tran_train_sorted_fn = 'tran_train_sorted.csv'\n",
    "tag_train_sorted_fn = 'tag_train_sorted.csv'\n",
    "\n",
    "op_origin_fn = 'op_origin.csv'\n",
    "tran_origin_fn = 'tran_origin.csv'\n",
    "tag_train_fts_fn = 'tag_fts.csv'\n",
    "round1_fts_fn = 'round1_fts.csv'\n",
    "\n",
    "op_train_new_file = data_path + op_train_new_fn\n",
    "tran_train_new_file = data_path + tran_train_new_fn\n",
    "tag_train_new_file = data_path + tag_train_new_fn\n",
    "op_train_sorted_file = data_path + op_train_sorted_fn\n",
    "tran_train_sorted_file = data_path + tran_train_sorted_fn\n",
    "tag_train_sorted_file = data_path + tag_train_sorted_fn\n",
    "\n",
    "op_origin_file = origin_path + op_origin_fn\n",
    "tran_origin_file = origin_path + tran_origin_fn\n",
    "tag_train_fts_file = fts_path + tag_train_fts_fn\n",
    "round1_fts_file = fts_path + round1_fts_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print('[info]:start read from op_train...')\n",
    "    op_train = pd.read_csv(op_origin_file).drop('Unnamed: 0', axis=1)\n",
    "    print('[info]:start read from tran_train...')\n",
    "    tran_train = pd.read_csv(tran_origin_file).drop('Unnamed: 0', axis=1)\n",
    "    print('[info]:start read from tag_train...')\n",
    "    tag_train = pd.read_csv(tag_train_sorted_file).drop('Unnamed: 0', axis=1)\n",
    "    return op_train, tran_train, tag_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_rd1 = pd.read_csv('../data/operation_round1_new.csv')\n",
    "tran_rd1 = pd.read_csv('../data/transaction_round1_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UID    31198\n",
       "Tag    31198\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.read_csv('../data/submit_sample.csv')\n",
    "s.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_test_data():\n",
    "    train_data = pd.read_csv(tag_train_fts_file)\n",
    "    test_data = pd.read_csv(round1_fts_file)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 引入train test数据\n",
    "train_data, test_data = load_train_test_data()\n",
    "train_data.drop(['Unnamed: 0'],axis=1, inplace=True)\n",
    "test_data.drop(['Unnamed: 0'],axis=1, inplace=True)\n",
    "train_data.fillna(-1, inplace=True)\n",
    "test_data.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = train_data['Tag']\n",
    "train = train_data.drop(['UID','Tag'],axis = 1)\n",
    "test_id = test_data['UID']\n",
    "test = test_data.drop(['UID','Tag'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.136765\tvalid_1's binary_logloss: 0.150917\n",
      "[100]\tvalid_0's binary_logloss: 0.136868\tvalid_1's binary_logloss: 0.151611\n",
      "Early stopping, best iteration is:\n",
      "[50]\tvalid_0's binary_logloss: 0.136765\tvalid_1's binary_logloss: 0.150917\n",
      "[0.15091651310605717]\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.137808\tvalid_1's binary_logloss: 0.147754\n",
      "[100]\tvalid_0's binary_logloss: 0.137432\tvalid_1's binary_logloss: 0.147265\n",
      "Early stopping, best iteration is:\n",
      "[60]\tvalid_0's binary_logloss: 0.137616\tvalid_1's binary_logloss: 0.147122\n",
      "[0.15091651310605717, 0.1471218244880291]\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.135\tvalid_1's binary_logloss: 0.162508\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.134868\tvalid_1's binary_logloss: 0.162446\n",
      "[0.15091651310605717, 0.1471218244880291, 0.16244550659082302]\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.136556\tvalid_1's binary_logloss: 0.158436\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.136257\tvalid_1's binary_logloss: 0.158401\n",
      "[0.15091651310605717, 0.1471218244880291, 0.16244550659082302, 0.15840059104190182]\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.138115\tvalid_1's binary_logloss: 0.152852\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid_0's binary_logloss: 0.138036\tvalid_1's binary_logloss: 0.152632\n",
      "[0.15091651310605717, 0.1471218244880291, 0.16244550659082302, 0.15840059104190182, 0.15263170903300696]\n"
     ]
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMClassifier(boosting_type='rf', num_leaves=100, reg_alpha=0.0, reg_lambda=1, max_depth=-1,\n",
    "    n_estimators=5000, objective='binary', subsample=0.7, colsample_bytree=0.77, subsample_freq=1, learning_rate=0.01,\n",
    "    random_state=2018, n_jobs=50, min_child_weight=4, min_child_samples=5, min_split_gain=0)\n",
    "skf = StratifiedKFold(n_splits=5, random_state=2018, shuffle=True)\n",
    "best_score = []\n",
    "\n",
    "oof_preds = np.zeros(train.shape[0])\n",
    "sub_preds = np.zeros(test_id.shape[0])\n",
    "\n",
    "for index, (train_index, test_index) in enumerate(skf.split(train, label)):\n",
    "    lgb_model.fit(train.iloc[train_index], label.iloc[train_index], verbose=50,\n",
    "                  eval_set=[(train.iloc[train_index], label.iloc[train_index]),\n",
    "                            (train.iloc[test_index], label.iloc[test_index])], early_stopping_rounds=50)\n",
    "    best_score.append(lgb_model.best_score_['valid_1']['binary_logloss'])\n",
    "    print(best_score)\n",
    "    oof_preds[test_index] = lgb_model.predict_proba(train.iloc[test_index], num_iteration=lgb_model.best_iteration_)[:,1]\n",
    "\n",
    "    test_pred = lgb_model.predict_proba(test, num_iteration=lgb_model.best_iteration_)[:, 1]\n",
    "    sub_preds += test_pred / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7347257876312718\n"
     ]
    }
   ],
   "source": [
    "m = tpr_weight_funtion(y_predict=oof_preds,y_true=label)\n",
    "print(m[1])\n",
    "submit = pd.read_csv('../data/submit_sample.csv')\n",
    "submit['Tag'] = sub_preds\n",
    "submit.to_csv('../data/sub/baseline_%s.csv'%str(m),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.read_csv('../data/sub/baseline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31198, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "op_train, tran_train, tag_train = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_mac1_dict = op_train['mac1'].value_counts().to_dict()\n",
    "op_ipsub_dict = op_train['ipsub'].value_counts().to_dict()\n",
    "tran_mac1_dict = tran_train['mac1'].value_counts().to_dict()\n",
    "tran_ip1sub_dict = tran_train['ip1_sub'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag(uid):\n",
    "    return tag_train[tag_train['UID'] == uid]\n",
    "\n",
    "def get_op(uid):\n",
    "    return op_train[op_train['UID'] == uid]\n",
    "\n",
    "def get_tran(uid):\n",
    "    return tran_train[tran_train['UID'] == uid]\n",
    "\n",
    "def get_value_counts(uid, train_data):\n",
    "    assert type(train_data) is pd.DataFrame\n",
    "    for c in list(train_data.columns):\n",
    "        print('[%r]'%c)\n",
    "        print(train_data[train_data['UID']==uid][c].value_counts())\n",
    "        print('====')\n",
    "        \n",
    "def oneday_cnt(tmp, gb_str, ft_str):\n",
    "    gb = tmp.groupby(gb_str)\n",
    "    top_oneday = []\n",
    "    for gb_key in gb.indices.keys():\n",
    "        cnt = 0\n",
    "        sub_gb = gb.get_group(gb_key)\n",
    "        value_counts = sub_gb[ft_str].value_counts()\n",
    "        if not value_counts.empty:\n",
    "            top_value = value_counts.sort_values(ascending=False).values[0]\n",
    "            top_mode = value_counts.sort_values(ascending=False).index[0]\n",
    "            top_oneday.append([gb_key, top_value, top_mode])\n",
    "    if not len(top_oneday):\n",
    "        top_oneday = [np.nan, np.nan, np.nan]\n",
    "    return top_oneday\n",
    "\n",
    "def topcnts_oneday(tmp, sub_str):\n",
    "    top_oneday = oneday_cnt(tmp, 'day', sub_str)\n",
    "    top_idx, top_cnt = 0, top_oneday[0][1]\n",
    "    for item in top_oneday:\n",
    "        if top_cnt < item[1]:\n",
    "            top_cnt = item[1]\n",
    "            top_idx += 1\n",
    "    top_day = top_oneday[top_idx][0]\n",
    "    top_value = top_oneday[top_idx][1]\n",
    "    top_mode = top_oneday[top_idx][2]\n",
    "    return top_day, top_value, top_mode\n",
    "        \n",
    "def op_times_per2min(tmp):\n",
    "    day_gb = tmp.groupby('day')\n",
    "    min2 = 120\n",
    "    over2min_rec = []\n",
    "    for gb_key in day_gb.indices.keys():\n",
    "        over2min_cnt = 0\n",
    "        sgb = day_gb.get_group(gb_key)\n",
    "        timestamp = list(sgb['timestamp'].get_values())\n",
    "        index, idx_max = 0, len(timestamp) - 1\n",
    "        now_t = timestamp[index]\n",
    "        for t in timestamp:\n",
    "            if timestamp[index] - now_t > min2:\n",
    "                now_t = timestamp[index]\n",
    "                over2min_cnt += 1\n",
    "            index += 1\n",
    "        over2min_rec.append(over2min_cnt)\n",
    "    return max(over2min_rec)\n",
    "\n",
    "def suc_rate(tmp):\n",
    "    for item in tmp['success'].value_counts().items():\n",
    "        if item[0] == 1:\n",
    "            suc_cnt = item[1]\n",
    "            break\n",
    "    suc_all = tmp['success'].count()\n",
    "    return suc_cnt/suc_all\n",
    "\n",
    "def fbfill_series(series):\n",
    "    series = series.copy()\n",
    "#     series.replace(0, np.nan, inplace=True)\n",
    "    new_s = series.fillna(method='bfill')\n",
    "    new_s = series.fillna(method='ffill')\n",
    "    return new_s\n",
    "\n",
    "def ft_change_cnt(tmp, ft_str):   \n",
    "    assert ft_str in tmp.keys()\n",
    "    \n",
    "    # 排除某一个ft下全部是NaN的情况\n",
    "    if len(tmp[ft_str][tmp[ft_str].notna()]):\n",
    "        series = fbfill_series(tmp[ft_str])\n",
    "        last_dc = series.get_values()[0]\n",
    "        dc_cnt = 0\n",
    "        for dc in series.get_values():\n",
    "            if last_dc != dc:\n",
    "                last_dc = dc\n",
    "                dc_cnt += 1\n",
    "        return dc_cnt\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def get_change_frq(tmp, ft_str):\n",
    "    frq = ft_change_cnt(tmp, ft_str)/tmp['day'].count()\n",
    "    if frq is not np.nan:\n",
    "        return float('%.2f' % frq)\n",
    "    else:\n",
    "        return np.nan\n",
    "       \n",
    "def ip_change_oneday_top(tmp, ip_ft_str):   \n",
    "    day_gb = tmp.groupby('day')\n",
    "    ip_rec = []\n",
    "    for gb_key in day_gb.indices.keys():\n",
    "        sgb = day_gb.get_group(gb_key)\n",
    "        series = fbfill_series(sgb[ip_ft_str])\n",
    "        last_ip = series.get_values()[0]\n",
    "        ip_cnt = 0\n",
    "        if np.isnan(last_ip):\n",
    "            ip_rec.append(np.nan)\n",
    "            continue\n",
    "        for ip in series.get_values():\n",
    "            if last_ip != ip:\n",
    "                last_ip = ip\n",
    "                ip_cnt += 1\n",
    "        ip_rec.append(ip_cnt)\n",
    "    return max(ip_rec)\n",
    "\n",
    "def top_type(tmp, ft_str):\n",
    "    value_counts = tmp[ft_str].value_counts()\n",
    "    if not value_counts.empty:\n",
    "        return value_counts.sort_values(ascending=False).index[0]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def top_value(tmp, ft_str):\n",
    "    value_counts = tmp[ft_str].value_counts()\n",
    "    if not value_counts.empty:\n",
    "        return value_counts.sort_values(ascending=False).values[0]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def get_frq(tmp, ft_str):\n",
    "    return float('%.2f' % (top_value(tmp, ft_str)/tmp['day'].count()))\n",
    "\n",
    "def top_op_mac1_in_diffUID_cnt(tmp):\n",
    "    tp = top_type(tmp, 'mac1')\n",
    "    if tp and tp in op_mac1_dict.keys():\n",
    "        return op_mac1_dict[tp]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def top_type_in_diffUID_cnt(tmp, ft_str, type_dict):\n",
    "    tp = top_type(tmp, ft_str)\n",
    "    if tp and tp in type_dict.keys():\n",
    "        return type_dict[tp]\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "op_gb = op_train.groupby('UID')\n",
    "drop_fts = ['mode','success','time','device1','device2','device_code1','device_code2','device_code3','mac1','ip1','ip2','ip1_sub','ip2_sub','timestamp']\n",
    "op_train_nf = op_gb.count().drop(drop_fts, axis='columns')\n",
    "op_train_nf.rename(columns={'day':'day_cnts'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_10001 = get_op(10001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# op\n",
    "op_feature = {}\n",
    "op_feature['UID'] = op_10001['UID'].values[0]\n",
    "op_feature['day_cnts'] = op_10001['day'].count()\n",
    "op_feature['op_top_appear_day'] = top_type(op_10001, 'day')\n",
    "op_feature['op_top_appear_day_cnt'] = top_value(op_10001, 'day')\n",
    "op_feature['op_times_per2min'] = op_times_per2min(op_10001)\n",
    "op_feature['mode_top_day_oneday'] = topcnts_oneday(op_10001, 'mode')[0] # 一天中某一操作类型次数最多的那一天\n",
    "op_feature['mode_top_cnt_oneday'] = topcnts_oneday(op_10001, 'mode')[1] # 一天中某一操作类型次数最多的次数\n",
    "op_feature['mode_top_type_oneday'] = topcnts_oneday(op_10001, 'mode')[2] # 一天中某一操作类型次数最多的类型\n",
    "op_feature['mode_cnt'] = top_value(op_10001, 'mode')\n",
    "op_feature['mode_rank1'] = top_type(op_10001, 'mode')\n",
    "op_feature['suc_rate'] = '%.2f' % (suc_rate(op_10001))\n",
    "op_feature['device_code_frq'] = get_change_frq(op_10001, 'device_code')\n",
    "op_feature['ip_change_frq'] = get_change_frq(op_10001, 'ip')\n",
    "op_feature['ip_change_oneday_top'] = ip_change_oneday_top(op_10001, 'ip')\n",
    "op_feature['ip_change_cnt'] = ft_change_cnt(op_10001, 'ip')\n",
    "op_feature['top_mac1_in_diffUID_cnt'] = top_type_in_diffUID_cnt(op_10001, 'mac1', op_mac1_dict)\n",
    "op_feature['top_ipsub_in_diffUID_cnt'] = top_type_in_diffUID_cnt(op_10001, 'ipsub', op_ipsub_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "features.append(op_feature)\n",
    "features = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_train_nf = op_train_nf.join(features)\n",
    "op_train_nf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_10001 = get_tran(10001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_10001 = get_tran(17520)\n",
    "tran_feature = {}\n",
    "tran_feature['UID'] = tran_10001['UID'].values[0]\n",
    "tran_feature['channel_top'] = top_type(tran_10001, 'channel')\n",
    "tran_feature['channel_top_frq'] = top_value(tran_10001, 'channel')\n",
    "tran_feature['tran_day_cnts'] = tran_10001['day'].count()\n",
    "tran_feature['tran_day_appear_top'] = top_type(tran_10001, 'day')\n",
    "tran_feature['tran_amt_frq'] = get_frq(tran_10001, 'trans_amt')\n",
    "tran_feature['tran_amt_top'] = top_type(tran_10001, 'trans_amt')\n",
    "tran_feature['tran_topcnts_oneday'] = topcnts_oneday(tran_10001, 'trans_amt')[1]\n",
    "tran_feature['tran_times_per2min'] = op_times_per2min(tran_10001)\n",
    "tran_feature['amt_src1_frq'] = get_change_frq(tran_10001, 'amt_src1')\n",
    "tran_feature['amt_src1_type_top'] = top_type(tran_10001, 'amt_src1')\n",
    "tran_feature['amt_src1_type_cnt'] = topcnts_oneday(tran_10001, 'amt_src1')[1]\n",
    "tran_feature['amt_src2_frq'] = get_change_frq(tran_10001, 'amt_src2')\n",
    "tran_feature['amt_src2_type_top'] = top_type(tran_10001, 'amt_src2')\n",
    "tran_feature['amt_src2_type_cnt'] = topcnts_oneday(tran_10001, 'amt_src2')[1]\n",
    "tran_feature['merchant_frq'] = get_change_frq(tran_10001, 'merchant')\n",
    "tran_feature['merchant_type_top'] = top_type(tran_10001, 'merchant')\n",
    "tran_feature['merchant_type_cnt'] = len(tran_10001['merchant'].value_counts()) # 商户标识类型总数\n",
    "tran_feature['code1_type_top'] = top_type(tran_10001, 'code1')\n",
    "tran_feature['code1_type_cnt'] = len(tran_10001['code1'].value_counts()) # 出现最多的商户子门店\n",
    "tran_feature['trans_type1_top_cnt'] = top_value(tran_10001, 'trans_type1')\n",
    "tran_feature['trans_type1_top_frq'] = get_frq(tran_10001, 'trans_type1')\n",
    "tran_feature['trans_type1_top'] = top_type(tran_10001, 'trans_type1')\n",
    "tran_feature['trans_type2_top_cnt'] = top_value(tran_10001, 'trans_type2')\n",
    "tran_feature['trans_type2_top_frq'] = get_frq(tran_10001, 'trans_type2')\n",
    "tran_feature['trans_type2_top'] = top_type(tran_10001, 'trans_type2')\n",
    "tran_feature['acc_id1_top_cnt'] = top_value(tran_10001, 'acc_id1')\n",
    "tran_feature['acc_id1_top_frq'] = get_frq(tran_10001, 'acc_id1')\n",
    "tran_feature['acc_id1_top'] = top_type(tran_10001, 'acc_id1')\n",
    "tran_feature['device_code_frq'] = get_change_frq(tran_10001, 'device_code')\n",
    "tran_feature['dev_name_frq'] = get_change_frq(tran_10001, 'device1')\n",
    "tran_feature['dev_type_frq'] = get_change_frq(tran_10001, 'device2')\n",
    "tran_feature['ip_change_oneday_top'] = ip_change_oneday_top(tran_10001, 'ip1')\n",
    "tran_feature['ip_change_frq'] = get_change_frq(tran_10001, 'ip1') # ip变化次数\n",
    "tran_feature['ip_change_times'] = ft_change_cnt(tran_10001, 'ip1') # ip变化次数\n",
    "tran_feature['top_mac1_in_diffUID_cnt'] = top_type_in_diffUID_cnt(tran_10001, 'mac1', tran_mac1_dict)\n",
    "tran_feature['top_ip1sub_in_diffUID_cnt'] = top_type_in_diffUID_cnt(tran_10001, 'ip1_sub', tran_ip1sub_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tran_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(op,trans,label):\n",
    "    for feature in op.columns[2:]:\n",
    "        label = label.merge(op.groupby(['UID'])[feature].count().reset_index(),on='UID',how='left')\n",
    "        label =label.merge(op.groupby(['UID'])[feature].nunique().reset_index(),on='UID',how='left')\n",
    "    \n",
    "    for feature in trans.columns[2:]:\n",
    "        if trans_train[feature].dtype == 'object':\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].count().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].nunique().reset_index(),on='UID',how='left')\n",
    "        else:\n",
    "            print(feature)\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].count().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].nunique().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].max().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].min().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].sum().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].mean().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].std().reset_index(),on='UID',how='left')\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_feature(op_train,trans_train,y)\n",
    "test = get_feature(op_test,trans_test,sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['UID','Tag'],axis = 1).fillna(-1)\n",
    "label = y['Tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test['UID']\n",
    "test = test.drop(['UID','Tag'],axis = 1).fillna(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
